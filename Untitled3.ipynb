{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWrOH/SaJKjZsystBxGwBS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chithraprasadn/trial/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJvjULEwmV0_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?id=1QdKB2YUZseVOoSH3LVVOHKA_7lq9Cb9-\")\n",
        "\n",
        "# Print the Shape of the DataFrame\n",
        "print(\"Shape of the DataFrame:\")\n",
        "print(df.shape)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Understand the data types of each column\n",
        "print(df.dtypes)\n",
        "\n",
        "# Get a sense of the distribution of numerical variables\n",
        "print(df.describe())\n",
        "\n",
        "# Get a sense of the distribution of categorical variables\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    print(df[col].value_counts())\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?id=1QdKB2YUZseVOoSH3LVVOHKA_7lq9Cb9-\")\n",
        "\n",
        "# Convert Dt_Customer to datetime\n",
        "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')\n",
        "\n",
        "# Visualize the distribution of customer incomes\n",
        "plt.figure(figsize=(10, 6), facecolor='white')\n",
        "sns.histplot(df['Income'], bins=30, kde=True)\n",
        "plt.title('Distribution of Customer Incomes')\n",
        "plt.xlabel('Income')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the relationship between Income and Amount Spent on Coke\n",
        "plt.figure(figsize=(10, 6), facecolor='white')\n",
        "sns.scatterplot(x='Income', y='MntCoke', data=df)\n",
        "plt.title('Income vs Amount Spent on Coke')\n",
        "plt.xlabel('Income')\n",
        "plt.ylabel('Amount Spent on Coke')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the distribution of key variables\n",
        "plt.figure(figsize=(10, 6), facecolor='white')\n",
        "sns.histplot(df['Year_Birth'], bins=30, kde=True)\n",
        "plt.title('Distribution of Customer Year of Birth')\n",
        "plt.xlabel('Year of Birth')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the distribution of the number of purchases made in stores\n",
        "plt.figure(figsize=(10, 6), facecolor='white')\n",
        "sns.histplot(df['NumStorePurchases'], bins=30, kde=False)\n",
        "plt.title('Distribution of Number of Store Purchases')\n",
        "plt.xlabel('Number of Store Purchases')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the relationship between Income and Total Amount Spent\n",
        "# Calculate total amount spent\n",
        "df['TotalSpent'] = df[['MntCoke', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']].sum(axis=1)\n",
        "plt.figure(figsize=(10, 6), facecolor='white')\n",
        "sns.scatterplot(x='Income', y='TotalSpent', data=df)\n",
        "plt.title('Income vs Total Amount Spent')\n",
        "plt.xlabel('Income')\n",
        "plt.ylabel('Total Amount Spent')\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "# Select relevant columns for income levels and product preferences\n",
        "income_and_products = df[['Income', 'MntCoke', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']]\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = income_and_products.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Between Income Levels and Product Preferences')\n",
        "plt.xlabel('Product Preferences')\n",
        "plt.ylabel('Income Levels')\n",
        "plt.show()\n",
        "\n",
        "print('Visualizations generated.')\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?id=1QdKB2YUZseVOoSH3LVVOHKA_7lq9Cb9-\", encoding='ascii')\n",
        "\n",
        "# Drop unnecessary columns\n",
        "unnecessary_columns = ['Unnamed: 0.1', 'Unnamed: 0', 'Z_CostContact', 'Z_Revenue']\n",
        "df.drop(columns=unnecessary_columns, inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "# Impute missing values in 'Income' with median\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df['Income'] = imputer.fit_transform(df[['Income']])\n",
        "\n",
        "# Feature Engineering\n",
        "# Convert Dt_Customer to datetime and extract year\n",
        "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')\n",
        "df['Customer_Year'] = df['Dt_Customer'].dt.year\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_features = ['Education', 'Marital_Status']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Normalize numerical features\n",
        "numerical_features = ['Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Recency', 'MntCoke', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Customer_Year']\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Combine transformations\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Apply transformations\n",
        "df_preprocessed = pd.DataFrame(preprocessor.fit_transform(df))\n",
        "\n",
        "print('Feature engineering, handling missing values, encoding, and normalization completed.')\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.impute import SimpleImputer\n",
        "from google.colab import files\n",
        "\n",
        "# Assuming you have loaded your data into a pandas dataframe called 'df'\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?id=1QdKB2YUZseVOoSH3LVVOHKA_7lq9Cb9-\", encoding='ascii')\n",
        "\n",
        "# Feature selection (choose features relevant to campaign acceptance)\n",
        "features = ['Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Recency', 'MntCoke', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
        "target = 'Response'  # Target variable for classification (campaign acceptance)\n",
        "\n",
        "# Split data into training and testing sets (around 80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values in the training data\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "# Classification Models\n",
        "\n",
        "# Model 1: Logistic Regression\n",
        "model_lr = LogisticRegression(solver='liblinear')\n",
        "model_lr.fit(X_train_imputed, y_train)\n",
        "\n",
        "# Model 2: Random Forest Classifier\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_rf.fit(X_train_imputed, y_train)\n",
        "\n",
        "# You can add more models here (e.g., Decision Tree)\n",
        "\n",
        "# Clustering Model (K-Means)\n",
        "\n",
        "# Define clustering features\n",
        "clustering_features = ['Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Recency', 'MntCoke', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
        "\n",
        "# Handle missing values in the clustering features\n",
        "X_cluster_imputed = imputer.fit_transform(df[clustering_features])\n",
        "\n",
        "# K-Means clustering with chosen k (replace 3 with your chosen value based on analysis)\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "kmeans.fit(X_cluster_imputed)\n",
        "\n",
        "# Add cluster labels to the dataframe\n",
        "df['cluster'] = kmeans.labels_\n",
        "\n",
        "# Now you have trained models and customer segments in 'df'\n",
        "print(\"Models trained and customer segmentation completed. Analyze results!\")\n",
        "\n",
        "# Assuming you have a dataframe 'df' with a cluster label column named 'cluster'\n",
        "\n",
        "print(df.groupby('cluster').mean())  # Descriptive statistics for each cluster\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Assuming you have trained classification and clustering models (replace with your model names)\n",
        "model_lr = LogisticRegression()  # Replace with your trained Logistic Regression model\n",
        "model_rf = RandomForestClassifier()  # Replace with your trained Random Forest model\n",
        "kmeans = KMeans(n_clusters=3)  # Replace with your trained KMeans model\n",
        "\n",
        "# Assuming you have predicted labels (y_pred) and true labels (y_test) for classification\n",
        "\n",
        "# Classification Evaluation\n",
        "\n",
        "# Fit the Logistic Regression model with training data\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy: Proportion of correctly predicted labels\n",
        "accuracy = accuracy_score(y_test, model_lr.predict(X_test))\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision: Ratio of true positives to all predicted positives\n",
        "precision = precision_score(y_test, model_lr.predict(X_test))\n",
        "print(f\"Logistic Regression Precision: {precision:.4f}\")\n",
        "\n",
        "# Recall: Ratio of true positives to all actual positives\n",
        "recall = recall_score(y_test, model_lr.predict(X_test))\n",
        "print(f\"Logistic Regression Recall: {recall:.4f}\")\n",
        "\n",
        "# F1-score: Harmonic mean of precision and recall\n",
        "f1 = f1_score(y_test, model_lr.predict(X_test))\n",
        "print(f\"Logistic Regression F1-score: {f1:.4f}\")\n",
        "\n",
        "# ROC AUC score: Area Under the Receiver Operating Characteristic Curve\n",
        "# (higher is better for binary classification)\n",
        "roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test)[:, 1])\n",
        "print(f\"Logistic Regression ROC AUC score: {roc_auc:.4f}\")\n",
        "\n",
        "# Repeat evaluation metrics for other classification models (model_rf)\n",
        "\n",
        "# Clustering Evaluation (assuming data used for clustering is 'df')\n",
        "\n",
        "# Fit the KMeans model with the clustering features\n",
        "kmeans.fit(df[clustering_features])\n",
        "\n",
        "# Silhouette score: Measures cluster cohesion and separation (higher is better)\n",
        "silhouette = silhouette_score(df[clustering_features], kmeans.predict(df[clustering_features]))\n",
        "print(f\"KMeans Silhouette Score: {silhouette:.4f}\")\n",
        "\n",
        "# Davies-Bouldin index: Measures cluster separation (lower is better)\n",
        "davies_bouldin = davies_bouldin_score(df[clustering_features], kmeans.predict(df[clustering_features]))\n",
        "print(f\"KMeans Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
        "\n",
        "# Visualize clusters (optional, using matplotlib)\n",
        "# ... (add your visualization code here)\n",
        "\n",
        "# Compare model performance based on metrics and choose the best performing ones\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"https://drive.google.com/uc?id=1QdKB2YUZseVOoSH3LVVOHKA_7lq9Cb9-\", encoding='ascii')\n",
        "\n",
        "# Define features and target\n",
        "features = ['Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Recency', 'MntCoke', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth']\n",
        "target = 'Response'\n",
        "\n",
        "# Split data for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model_lr = LogisticRegression(solver='liblinear').fit(X_train_imputed, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = model_lr.score(X_test_imputed, y_test)\n",
        "print(f'Logistic Regression Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygz20XAvLnun",
        "outputId": "08b3ad07-facb-44f6-b078-cd9c93238ae6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.8542\n"
          ]
        }
      ]
    }
  ]
}